# -*- coding: utf-8 -*-
"""Project Done sentiment analysis with TWITT_live.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Y73C6huEZz3n6RenMAl5auC7GLYYCVzk
"""

import pandas as pd
import numpy as np
import re
import seaborn as sns
import matplotlib.pyplot as plt
import nltk
nltk.download('stopwords')
import nltk
nltk.download('punkt')
from matplotlib import style
style.use('ggplot')
from textblob import TextBlob
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))
from wordcloud import WordCloud
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay

df = pd.read_csv('vaccination_all_tweets.csv')

"""# New section"""

df.head()

df.info()

df.isnull().sum()

df.columns

text_df = df.drop(['id', 'user_name', 'user_location', 'user_description', 'user_created',
       'user_followers', 'user_friends', 'user_favourites', 'user_verified',
       'date', 'hashtags', 'source', 'retweets', 'favorites',
       'is_retweet'], axis=1)
text_df.head()

print(text_df['text'].iloc[0],"\n")
print(text_df['text'].iloc[1],"\n")
print(text_df['text'].iloc[2],"\n")
print(text_df['text'].iloc[3],"\n")
print(text_df['text'].iloc[4],"\n")

text_df.info()

def data_processing(text):
    text = text.lower()
    text = re.sub(r"https\S+|www\S+https\S+", '',text, flags=re.MULTILINE)
    text = re.sub(r'\@w+|\#','',text)
    text = re.sub(r'[^\w\s]','',text)
    text_tokens = word_tokenize(text)
    filtered_text = [w for w in text_tokens if not w in stop_words]
    return " ".join(filtered_text)

text_df.text = text_df['text'].apply(data_processing)

text_df = text_df.drop_duplicates('text')

stemmer = PorterStemmer()
def stemming(data):
    text = [stemmer.stem(word) for word in data]
    return data

text_df['text'] = text_df['text'].apply(lambda x: stemming(x))

text_df.head()

print(text_df['text'].iloc[0],"\n")
print(text_df['text'].iloc[1],"\n")
print(text_df['text'].iloc[2],"\n")
print(text_df['text'].iloc[3],"\n")
print(text_df['text'].iloc[4],"\n")

text_df.info()

def polarity(text):
    return TextBlob(text).sentiment.polarity

text_df['polarity'] = text_df['text'].apply(polarity)

text_df.head(10)

def sentiment(label):
    if label <0:
        return "Negative"
    elif label ==0:
        return "Neutral"
    elif label>0:
        return "Positive"

text_df['sentiment'] = text_df['polarity'].apply(sentiment)

text_df.head()

fig = plt.figure(figsize=(5,5))
sns.countplot(x='sentiment', data = text_df)

fig = plt.figure(figsize=(7,7))
colors = ("yellowgreen", "gold", "red")
wp = {'linewidth':2, 'edgecolor':"black"}
tags = text_df['sentiment'].value_counts()
explode = (0.1,0.1,0.1)
tags.plot(kind='pie', autopct='%1.1f%%', shadow=True, colors = colors,
         startangle=90, wedgeprops = wp, explode = explode, label='')
plt.title('Distribution of sentiments')

pos_tweets = text_df[text_df.sentiment == 'Positive']
pos_tweets = pos_tweets.sort_values(['polarity'], ascending= False)
pos_tweets.head()

text = ' '.join([word for word in pos_tweets['text']])
plt.figure(figsize=(20,15), facecolor='None')
wordcloud = WordCloud(max_words=500, width=1600, height=800).generate(text)
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.title('Most frequent words in positive tweets', fontsize=19)
plt.show()

neg_tweets = text_df[text_df.sentiment == 'Negative']
neg_tweets = neg_tweets.sort_values(['polarity'], ascending= False)
neg_tweets.head()

text = ' '.join([word for word in neg_tweets['text']])
plt.figure(figsize=(20,15), facecolor='None')
wordcloud = WordCloud(max_words=500, width=1600, height=800).generate(text)
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.title('Most frequent words in negative tweets', fontsize=19)
plt.show()

neutral_tweets = text_df[text_df.sentiment == 'Neutral']
neutral_tweets = neutral_tweets.sort_values(['polarity'], ascending= False)
neutral_tweets.head()

text = ' '.join([word for word in neutral_tweets['text']])
plt.figure(figsize=(20,15), facecolor='None')
wordcloud = WordCloud(max_words=500, width=1600, height=800).generate(text)
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.title('Most frequent words in neutral tweets', fontsize=19)
plt.show()

vect = CountVectorizer(ngram_range=(1,2)).fit(text_df['text'])

feature_names = vect.get_feature_names()
print("Number of features: {}\n".format(len(feature_names)))
print("First 20 features:\n {}".format(feature_names[:20]))

X = text_df['text']
Y = text_df['sentiment']
X = vect.transform(X)

x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

print("Size of x_train:", (x_train.shape))
print("Size of y_train:", (y_train.shape))
print("Size of x_test:", (x_test.shape))
print("Size of y_test:", (y_test.shape))

import warnings
warnings.filterwarnings('ignore')

logreg = LogisticRegression()
logreg.fit(x_train, y_train)
logreg_pred = logreg.predict(x_test)
logreg_acc = accuracy_score(logreg_pred, y_test)
print("Test accuracy: {:.2f}%".format(logreg_acc*100))

print(confusion_matrix(y_test, logreg_pred))
print("\n")
print(classification_report(y_test, logreg_pred))

style.use('classic')
cm = confusion_matrix(y_test, logreg_pred, labels=logreg.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels=logreg.classes_)
disp.plot()

from sklearn.model_selection import GridSearchCV

param_grid={'C':[0.001, 0.01, 0.1, 1, 10]}
grid = GridSearchCV(LogisticRegression(), param_grid)
grid.fit(x_train, y_train)

print("Best parameters:", grid.best_params_)

y_pred = grid.predict(x_test)

logreg_acc = accuracy_score(y_pred, y_test)
print("Test accuracy: {:.2f}%".format(logreg_acc*100))

print(confusion_matrix(y_test, y_pred))
print("\n")
print(classification_report(y_test, y_pred))

from sklearn.svm import LinearSVC

SVCmodel = LinearSVC()
SVCmodel.fit(x_train, y_train)

svc_pred = SVCmodel.predict(x_test)
svc_acc = accuracy_score(svc_pred, y_test)
print("test accuracy: {:.2f}%".format(svc_acc*100))

print(confusion_matrix(y_test, svc_pred))
print("\n")
print(classification_report(y_test, svc_pred))

grid = {
    'C':[0.01, 0.1, 1, 10],
    'kernel':["linear","poly","rbf","sigmoid"],
    'degree':[1,3,5,7],
    'gamma':[0.01,1]
}
grid = GridSearchCV(SVCmodel, param_grid)
grid.fit(x_train, y_train)

print("Best parameter:", grid.best_params_)

y_pred = grid.predict(x_test)

logreg_acc = accuracy_score(y_pred, y_test)
print("Test accuracy: {:.2f}%".format(logreg_acc*100))

print(confusion_matrix(y_test, y_pred))
print("\n")
print(classification_report(y_test, y_pred))

"""Additional code to extract data form twitter using twitter api"""

# Commented out IPython magic to ensure Python compatibility.
#NEW CODE
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
import pandas as pd
import numpy as np
import re
import tweepy
import warnings
warnings.filterwarnings('ignore')

from tweepy.auth import OAuthHandler
import nltk
nltk.download('punkt')
nltk.download('stopwords')
from nltk.corpus import stopwords 
from textblob import *
from wordcloud import *

# Importing the keys from twitter api
bearer="AAAAAAAAAAAAAAAAAAAAAM%2FSbgEAAAAA5yJIK8gOdJxTNgAxleoWxzUIARo%3DY2EbYNbTEbfTT0zLd5Ln2K884lXiB63gcc4P3Q0pI2VZtyfJ9n"
consumerKey = "OfKvVZna3qyhTzCtvgGBurKfo"
consumerSecret = "yzUgAE31FaBSKG13g9CfeU9vWrNk7kZPagk6a1ScNyQHZDRppa"
accessToken = "1516235002383458306-WzxhekIf5uduHsU3CbqCBDfTDB4U3B"
accessTokenSecret = "vNHNAjdPdE217F0sodfFD5ifIR2QEQKuMGppgz2RYdQi2"

#authenticateing the keys to access realtime data from twitter
authenticate = OAuthHandler(consumerKey, consumerSecret)
authenticate.set_access_token(accessToken, accessTokenSecret)
api=tweepy.API(authenticate, wait_on_rate_limit=True)

#change screen name to change the tweet
posts = api.user_timeline(screen_name="srilanka", count=100, lang="en", tweet_mode="extended")
print("Show the 5 recent tweets: \n")
i=1
for tweet in posts[0:5]:
    print(str(i)+')'+tweet.full_text+'\n')
    i=i+1

#Create a dataframe with a column called tweets and id
df=pd.DataFrame([i+1 for i in range(100)],columns=['id'])
df['tweet']=[tweet.full_text.lower() for tweet in posts]
#show the first 5 rows of data
df.head()

#Remove_pattern function is used to remove the usernames, links and other special characters except hash tags
# removes pattern in the input text
def remove_pattern(input_txt, pattern):
    r = re.findall(pattern, input_txt)
    for word in r:
        input_txt = re.sub(word, "", input_txt)
    return input_txt

#Data clean the tweet column and create hash_tweet column with hashtags and tweets

# remove twitter handles (@user)
df['clean_tweet'] = np.vectorize(remove_pattern)(df['tweet'], "@[\w]*")
# remove links (https)
df['clean_tweet'] = np.vectorize(remove_pattern)(df['tweet'], "https[\w]*")
# remove all characters except # and alphabets
df['clean_tweet'] = df['clean_tweet'].str.replace("[^a-zA-Z#]", " ")
# remove short words that do not affect the polarity
df['clean_tweet'] = df['clean_tweet'].apply(lambda x: " ".join([w for w in x.split() if len(w)>3]))
df.head()
# individual words considered as tokens
tokenized_tweet = df['clean_tweet'].apply(nltk.word_tokenize)
stop = stopwords.words('english')
tokenized_tweet=tokenized_tweet.apply(lambda x: [item for item in x if item not in stop])
for i in range(len(tokenized_tweet)):
    tokenized_tweet[i] = " ".join(tokenized_tweet[i])
df['clean_tweet'] = tokenized_tweet
df.head()

#func to plot the wordcloud graph
def graph(word):
    wordcloud = WordCloud(width=800, height=500, random_state=42, max_font_size=100).generate(word)
    plt.figure(figsize=(15,8))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.show()

#visualize the frequent word
all_words = " ".join([sentence for sentence in df['clean_tweet']])
graph(all_words)

#FIND IF A TWEET IS NEGATIVE POSITIVE

#function to get polarity
def getPolarity(text):
    return TextBlob(text).sentiment.polarity
#Create two columns subjectivity and polarity
df['Polarity']=df['clean_tweet'].apply(getPolarity)
#show the new dataframe
df.head()

#Use polarity column to analyse nature of tweets

#create a function to compute negative, neutral and positve analysis
def getAnalysis(score):
    if score<0:
        return 'Negative'
    elif score==0:
        return 'Neutral'
    else:
        return 'Positive'    
df['Analysis']=df['Polarity'].apply(getAnalysis)
#show the dataframe
df.head()

pos_words = " ".join([sentence for sentence in df['clean_tweet'][df['Analysis']=='Positive']])
# plot the graph
graph(pos_words)

# frequent words visualization for -ve
neg_words = " ".join([sentence for sentence in df['clean_tweet'][df['Analysis']=='Negative']])
# plot the graph
graph(neg_words)

# frequent words visualization for neutral
neu_words = " ".join([sentence for sentence in df['clean_tweet'][df['Analysis']=='Neutral']])
# plot the graph
graph(neu_words)

#Find the percentage of each type of tweet
def percentage(vibe):
    pertweets= df[df.Analysis ==vibe]
    pertweets = pertweets['tweet']
    vibeval=round(pertweets.shape[0]/df.shape[0]*100,1)
    print(vibe,"Tweets:",end=" ")
    return vibeval

#Print the percentage values
positive=percentage('Positive')
print(positive)
negative=percentage('Negative')
print(negative)
neutral=percentage('Neutral')
print(neutral)

#Plot the positive, neutral and negative tweets in a bar graph

#show the value counts
df['Analysis'].value_counts()
plt.title('Sentiment Analysis')
plt.xlabel('Sentiment')
plt.ylabel('Counts')
df['Analysis'].value_counts().plot(kind='bar',color='red')
plt.show()

#Exploratory Data Analysis using the hashtags

# extract the hashtag
def hashtag_extract(tweets):
    hashtags = []
    # loop words in the tweet
    for tweet in tweets:
        ht = re.findall(r"#(\w+)", tweet)
        hashtags.append(ht)
    return hashtags

#Extracting hashtags from the hash_tweet considering analysis column

# extract hashtags from positive tweets
ht_positive = hashtag_extract(df['tweet'][df['Analysis']=='Positive'])

# extract hashtags from negative tweets
ht_negative = hashtag_extract(df['tweet'][df['Analysis']=='Negative'])

# extract hashtags from negative tweets
ht_neutral = hashtag_extract(df['tweet'][df['Analysis']=='Neutral'])

# unnest list
#Remove the empty list items within the list
ht_positive = sum(ht_positive, [])
ht_negative = sum(ht_negative, [])
ht_neutral = sum(ht_neutral, [])
ht_positive[:5]  #checking any one of them to see the list

#Function to plot the bargraph for top 10 hashtags along with the dataframe
def pdframe(vibe):
    freq = nltk.FreqDist(vibe)
    d = pd.DataFrame({'Hashtag': list(freq.keys()),'Count': list(freq.values())})  
    # select top 10 hashtags
    d = d.nlargest(columns='Count', n=10)
    plt.figure(figsize=(20,9))
    sns.barplot(data=d, x='Hashtag', y='Count')
    plt.show()  
    return d

#Plot for top 10 hash tags in positive tweets
print('Positive tweets based on hashtags')
freq_pos = pdframe(ht_positive)
freq_pos

#Plot for top 10 hash tags in negative tweets
print('Negative tweets based on hashtags')
freq_neg = pdframe(ht_negative)
freq_neg

#Plot for top 10 hash tags in neutral tweets
print('Neutral tweets based on hashtags')
freq_neu = pdframe(ht_neutral)
freq_neu